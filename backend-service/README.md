# moda.STUDIO åç«¯æ™ºèƒ½å®¢æœæœåŠ¡

è¿™æ˜¯ moda.STUDIO çš„ç‹¬ç«‹åç«¯å¾®æœåŠ¡ï¼Œä¸“é—¨ç”¨äºå¤„ç?Vertex AI çš„æ™ºèƒ½å®¢æœåŠŸèƒ½ã€?
## å¿«é€Ÿå¼€å§?
### æœ¬åœ°å¼€å?
```bash
# 1. è¿›å…¥ç›®å½•
cd backend-service

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate  # æˆ?Windows: venv\Scripts\activate

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 4. åˆ›å»º .env æ–‡ä»¶
cp .env.example .env

# 5. è¿è¡Œåº”ç”¨
python app.py
```

åº”ç”¨å°†åœ¨ `http://localhost:8080` å¯åŠ¨ã€?
### ä½¿ç”¨ Docker

```bash
# æ„å»ºé•œåƒ
docker build -t moda.STUDIO-backend .

# è¿è¡Œå®¹å™¨
docker run -p 8080:8080 \
  -e GCP_PROJECT_ID=gen-lang-client-0654563230 \
  -e GCP_REGION=us-central1 \
  moda.STUDIO-backend
```

### åœ?Google Cloud Run éƒ¨ç½²

```bash
# æ¨é€åˆ° GCR
docker push gcr.io/gen-lang-client-0654563230/moda.STUDIO-backend

# éƒ¨ç½²åˆ?Cloud Run
gcloud run deploy moda.STUDIO-backend \
  --image gcr.io/gen-lang-client-0654563230/moda.STUDIO-backend \
  --region us-central1 \
  --allow-unauthenticated \
  --set-env-vars="GCP_PROJECT_ID=gen-lang-client-0654563230,GCP_REGION=us-central1"
```

### åœ?GCE è™šæ‹Ÿæœºéƒ¨ç½?
```bash
# æ–¹æ³• 1ï¼šä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼?gcloud compute instance-templates create moda.STUDIO-backend-template \
  --machine-type=e2-medium \
  --scopes=https://www.googleapis.com/auth/cloud-platform \
  --metadata-from-file=startup-script=backend_startup_script.sh

# åˆ›å»ºå®ä¾‹ç»?gcloud compute instance-groups managed create moda.STUDIO-backend-group \
  --base-instance-name=moda.STUDIO-backend \
  --template=moda.STUDIO-backend-template \
  --size=1 \
  --region=us-central1

# æ–¹æ³• 2ï¼šæ‰‹åŠ¨è¿è¡Œå¯åŠ¨è„šæœ?curl https://raw.githubusercontent.com/v3ai2026/moda.STUDIO-v9.9/main/backend-service/backend_startup_script.sh | bash
```

## API æ–‡æ¡£

### 1. å¥åº·æ£€æŸ?
```bash
GET /health
```

**å“åº”:**
```json
{
  "status": "healthy"
}
```

### 2. ä¸»é¡µ

```bash
GET /
```

**å“åº”:**
```json
{
  "service": "moda.STUDIO Backend Service",
  "version": "1.0",
  "status": "running",
  "endpoints": {
    "/health": "å¥åº·æ£€æŸ?,
    "/api/chat": "æ™ºèƒ½å®¢æœå¯¹è¯",
    "/api/v1/chat": "å®¢æœå¯¹è¯ï¼ˆV1 APIï¼?
  }
}
```

### 3. æ™ºèƒ½å®¢æœå¯¹è¯

```bash
POST /api/chat
Content-Type: application/json

{
  "question": "ä½ ä»¬çš„äº§å“ä»·æ ¼æ˜¯å¤šå°‘ï¼?,
  "history": []  // å¯é€‰ï¼šå¯¹è¯å†å²
}
```

**å“åº”:**
```json
{
  "status": "success",
  "answer": "æ„Ÿè°¢æ‚¨çš„æé—®ï¼æˆ‘ä»¬æä¾›å¤šç§ä»·æ ¼æ–¹æ¡?..",
  "timestamp": "2026-01-03T12:00:00Z",
  "model": "gemini-pro"
}
```

### 4. åˆ—å‡ºå¯ç”¨æ¨¡å‹

```bash
GET /api/models
```

**å“åº”:**
```json
{
  "status": "success",
  "models": [
    {
      "id": "gemini-pro",
      "name": "Gemini Pro",
      "type": "text-generation"
    },
    {
      "id": "gemini-pro-vision",
      "name": "Gemini Pro Vision",
      "type": "multimodal"
    }
  ],
  "default": "gemini-pro"
}
```

## ç¯å¢ƒå˜é‡

| å˜é‡ | é»˜è®¤å€?| è¯´æ˜ |
|------|--------|------|
| `GCP_PROJECT_ID` | `gen-lang-client-0654563230` | Google Cloud é¡¹ç›® ID |
| `GCP_REGION` | `us-central1` | Vertex AI åŒºåŸŸ |
| `MODEL_ID` | `gemini-pro` | ä½¿ç”¨çš?AI æ¨¡å‹ |
| `PORT` | `8080` | ç›‘å¬ç«¯å£ |
| `FLASK_ENV` | `production` | Flask ç¯å¢ƒ |

## ä¸?moda.STUDIO ä¸»åº”ç”¨é›†æˆ?
åœ?moda.STUDIO Laravel åç«¯ä¸­è°ƒç”¨æ­¤æœåŠ¡ï¼?
```php
// app/Services/Ai/VertexAIService.php
namespace App\Services\Ai;

use Illuminate\Support\Facades\Http;

class VertexAIService
{
    protected $backendUrl = 'http://backend-service:8080';
    
    public function chat(string $question)
    {
        $response = Http::post("{$this->backendUrl}/api/chat", [
            'question' => $question,
        ]);
        
        return $response->json('answer');
    }
}

// åœ¨æ§åˆ¶å™¨ä¸­ä½¿ç”?public function chatWithAI(Request $request)
{
    $service = app(VertexAIService::class);
    $answer = $service->chat($request->input('question'));
    
    return response()->json(['answer' => $answer]);
}
```

## ç›‘æ§å’Œæ—¥å¿?
### æŸ¥çœ‹æœåŠ¡æ—¥å¿—

```bash
# Systemd æ—¥å¿—
journalctl -u moda.STUDIO-backend -f

# Docker æ—¥å¿—
docker logs -f <container_id>

# Cloud Run æ—¥å¿—
gcloud run logs read moda.STUDIO-backend --region=us-central1 --limit=50
```

### ç›‘æ§æŒ‡æ ‡

åœ?Google Cloud Console ä¸­æŸ¥çœ‹ï¼š
- **Cloud Logging**: åº”ç”¨æ—¥å¿—
- **Cloud Monitoring**: CPUã€å†…å­˜ã€è¯·æ±‚æ•°ç­‰æŒ‡æ ?- **Cloud Trace**: è¯·æ±‚è¿½è¸ª

## æ•…éšœæ’é™¤

### é—®é¢˜ 1: Vertex AI API æœªå¯ç”?
```bash
gcloud services enable aiplatform.googleapis.com --project=gen-lang-client-0654563230
```

### é—®é¢˜ 2: æƒé™ä¸è¶³

ç¡®ä¿æœåŠ¡è´¦å·æœ‰ä»¥ä¸‹æƒé™ï¼š
- `aiplatform.user`
- `aiplatform.serviceAgent`

```bash
gcloud projects add-iam-policy-binding gen-lang-client-0654563230 \
  --member="serviceAccount:your-sa@gen-lang-client-0654563230.iam.gserviceaccount.com" \
  --role="roles/aiplatform.user"
```

### é—®é¢˜ 3: è¯·æ±‚è¶…æ—¶

å¢åŠ  Gunicorn è¶…æ—¶æ—¶é—´ï¼?```bash
gunicorn --timeout 300 app:app
```

## æ€§èƒ½ä¼˜åŒ–

1. **ä½¿ç”¨è¿æ¥æ±?*: å¤ç”¨ Vertex AI è¿æ¥
2. **ç¼“å­˜**: å¯¹å¸¸è§é—®é¢˜è¿›è¡Œç¼“å­?3. **å¼‚æ­¥å¤„ç†**: ä½¿ç”¨ Celery å¤„ç†è€—æ—¶æ“ä½œ
4. **è´Ÿè½½å‡è¡¡**: ä½¿ç”¨ Google Cloud Load Balancer åˆ†é…æµé‡

## å®‰å…¨æ€?
- âœ?åœ?VPC ä¸­è¿è¡Œï¼ˆä¸ç›´æ¥æš´éœ²åˆ°äº’è”ç½‘ï¼‰
- âœ?ä½¿ç”¨æœåŠ¡è´¦å·è®¤è¯ï¼ˆæ— éœ€ API å¯†é’¥ï¼?- âœ?HTTPS åŠ å¯†ï¼ˆé€šè¿‡ Cloud Armorï¼?- âœ?é€Ÿç‡é™åˆ¶ï¼ˆåœ¨ Cloud Load Balancer å±‚ï¼‰

## ç›¸å…³æ–‡æ¡£

- [Vertex AI æ–‡æ¡£](https://cloud.google.com/vertex-ai/docs)
- [Gemini API æŒ‡å—](https://ai.google.dev)
- [Google Cloud éƒ¨ç½²æŒ‡å—](https://cloud.google.com/docs)
- [Flask æ–‡æ¡£](https://flask.palletsprojects.com/)

## è®¸å¯è¯?
MIT License
