# app.py - moda.STUDIO åç«¯æ™ºèƒ½å®¢æœæœåŠ¡ï¼ˆä¼˜åŒ–ç‰ˆ v2.0ï¼?
from flask import Flask, request, jsonify
from google.cloud import aiplatform
from vertexai.preview.generative_models import GenerativeModel, Content, Part
import os
import logging
import json
import time
import hashlib
import threading
from datetime import datetime
from functools import wraps
from collections import defaultdict
from typing import Optional

app = Flask(__name__)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ä»ç¯å¢ƒå˜é‡è·å–é…ç½?PROJECT_ID = os.environ.get('GCP_PROJECT_ID', 'gen-lang-client-0654563230')
REGION = os.environ.get('GCP_REGION', 'us-central1')
MODEL_ID = os.environ.get('MODEL_ID', 'gemini-2.0-flash')
PORT = int(os.environ.get('PORT', 8080))

# ç¼“å­˜é…ç½®
CACHE_TTL = int(os.environ.get('CACHE_TTL', 3600))
MAX_CACHE_SIZE = int(os.environ.get('MAX_CACHE_SIZE', 500))
MAX_REQUESTS_PER_MINUTE = int(os.environ.get('MAX_REQUESTS_PER_MINUTE', 60))

# ==================== ç®€å•ç¼“å­?====================
class Cache:
    def __init__(self):
        self.data = {}
        self.times = {}
        self.lock = threading.Lock()
    
    def key(self, q):
        return hashlib.md5(q.encode()).hexdigest()
    
    def get(self, question):
        k = self.key(question)
        with self.lock:
            if k in self.data and time.time() - self.times[k] < CACHE_TTL:
                return self.data[k]
        return None
    
    def set(self, question, answer):
        k = self.key(question)
        with self.lock:
            if len(self.data) >= MAX_CACHE_SIZE:
                oldest = min(self.times, key=self.times.get, default=k)
                if oldest in self.data:
                    del self.data[oldest]
                    del self.times[oldest]
            self.data[k] = answer
            self.times[k] = time.time()

cache = Cache()

# ==================== é€Ÿç‡é™åˆ¶ ====================
class RateLimit:
    def __init__(self, max_per_min=60):
        self.reqs = defaultdict(list)
        self.max = max_per_min
        self.lock = threading.Lock()
    
    def allow(self, client_id):
        now = time.time()
        with self.lock:
            self.reqs[client_id] = [t for t in self.reqs[client_id] if now - t < 60]
            if len(self.reqs[client_id]) < self.max:
                self.reqs[client_id].append(now)
                return True
        return False

limiter = RateLimit(max_per_min=MAX_REQUESTS_PER_MINUTE)

def rate_limit_check(f):
    @wraps(f)
    def wrapped(*args, **kwargs):
        client_id = request.remote_addr or 'unknown'
        if not limiter.allow(client_id):
            return jsonify({"error": "Too many requests. Try again later."}), 429
        return f(*args, **kwargs)
    return wrapped

# ==================== åˆå§‹åŒ?Vertex AI ====================
try:
    aiplatform.init(project=PROJECT_ID, location=REGION)
    logger.info(f"âœ?Vertex AI åˆå§‹åŒ–æˆåŠ? {PROJECT_ID}/{REGION}/{MODEL_ID}")
except Exception as e:
    logger.error(f"â?Vertex AI åˆå§‹åŒ–å¤±è´? {e}")

# ==================== AI æ ¸å¿ƒå‡½æ•° ====================
def get_ai_response(user_question, conversation_history=None):
    """
    è°ƒç”¨ Vertex AI è·å–å›å¤
    """
    try:
        # 1. æ£€æŸ¥ç¼“å­?        cached = cache.get(user_question)
        if cached:
            logger.info(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­: {user_question[:30]}...")
            return cached
        
        # 2. åˆå§‹åŒ–æ¨¡å?        model = GenerativeModel(MODEL_ID)
        
        # 3. ç³»ç»Ÿæç¤º
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½å®¢æœåŠ©æ‰‹ã€‚ä½ çš„èŒè´£æ˜¯ï¼?1. å‹å¥½ã€ç¤¼è²Œåœ°å›ç­”ç”¨æˆ·çš„é—®é¢?2. æä¾›å‡†ç¡®å’Œæœ‰å¸®åŠ©çš„ä¿¡æ?3. å¦‚æœä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯šå®åœ°è¯´æ˜å¹¶å»ºè®®ç”¨æˆ·è”ç³»äººå·¥å®¢æœ
4. ä½¿ç”¨ç”¨æˆ·çš„è¯­è¨€ï¼ˆä¸­æ–‡æˆ–è‹±æ–‡ï¼‰è¿›è¡Œå›å¤?5. å›å¤åº”ç®€æ´æ˜äº†ï¼Œä¸è¶…è¿?00å­?""
        
        # 4. æ„é€ è¯·æ±‚å†…å®?        if conversation_history:
            contents = conversation_history + [
                Content(role="user", parts=[Part.from_text(user_question)])
            ]
        else:
            contents = [
                Content(role="user", parts=[Part.from_text(system_prompt + "\n\nç”¨æˆ·æé—®: " + user_question)])
            ]
        
        # 5. è°ƒç”¨ Vertex AI
        logger.info(f"ğŸ“¤ å‘é€é—®é¢˜åˆ° Vertex AI: {user_question[:50]}...")
        response = model.generate_content(
            contents=contents,
            generation_config={
                "max_output_tokens": 500,
                "temperature": 0.7,
                "top_p": 0.8,
            }
        )
        
        answer = response.text
        logger.info(f"ğŸ“¥ æ”¶åˆ° AI å›å¤: {answer[:50]}...")
        
        # 6. ç¼“å­˜ç»“æœ
        cache.set(user_question, answer)
        
        return answer
        
    except Exception as e:
        logger.error(f"â?è°ƒç”¨ Vertex AI å¤±è´¥: {e}")
        return "æŠ±æ­‰ï¼Œæ™ºèƒ½å®¢æœç›®å‰æ— æ³•å“åº”ã€‚è¯·ç¨åå†è¯•æˆ–è”ç³»äººå·¥å®¢æœã€?

# ==================== è·¯ç”± ====================
@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚?- Google Cloud Run ä½¿ç”¨"""
    return jsonify({"status": "healthy"}), 200

@app.route('/', methods=['GET'])
def home():
    """ä¸»é¡µ"""
    return jsonify({
        "service": "moda.STUDIO Backend Service",
        "version": "2.0",
        "status": "running",
        "model": MODEL_ID,
        "endpoints": {
            "/health": "å¥åº·æ£€æŸ?,
            "/api/chat": "æ™ºèƒ½å®¢æœå¯¹è¯",
            "/api/v1/chat": "å®¢æœå¯¹è¯ï¼ˆV1 APIï¼?,
            "/api/models": "æ¨¡å‹åˆ—è¡¨"
        }
    }), 200

@app.route('/api/chat', methods=['POST'])
@rate_limit_check
def chat():
    """
    å¤„ç†å®¢æœå¯¹è¯è¯·æ±‚
    
    è¯·æ±‚æ ¼å¼:
    {
        "question": "ç”¨æˆ·çš„é—®é¢?,
        "history": [...]  // å¯é€‰ï¼šå¯¹è¯å†å²
    }
    
    å“åº”æ ¼å¼:
    {
        "status": "success",
        "answer": "AIçš„å›å¤?,
        "timestamp": "2026-01-03T12:00:00Z"
    }
    """
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({"error": "è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©?}), 400
        
        user_question = data.get('question', '').strip()
        conversation_history = data.get('history', None)
        
        if not user_question:
            return jsonify({"error": "question å­—æ®µä¸èƒ½ä¸ºç©º"}), 400
        
        logger.info(f"ğŸ’¬ å¤„ç†å¯¹è¯è¯·æ±‚: {user_question[:30]}...")
        
        # è°ƒç”¨ AI è·å–å›å¤
        ai_answer = get_ai_response(user_question, conversation_history)
        
        return jsonify({
            "status": "success",
            "answer": ai_answer,
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "model": MODEL_ID
        }), 200
        
    except Exception as e:
        logger.error(f"â?å¤„ç†å¯¹è¯æ—¶å‡ºé”? {e}")
        return jsonify({
            "status": "error",
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }), 500

@app.route('/api/v1/chat', methods=['POST'])
@rate_limit_check
def chat_v1():
    """V1 API - ä¸?/api/chat ç›¸åŒ"""
    return chat()

@app.route('/api/models', methods=['GET'])
def list_models():
    """åˆ—å‡ºå¯ç”¨çš„æ¨¡å?""
    try:
        models = [
            {"id": "gemini-2.0-flash", "name": "Gemini 2.0 Flash", "type": "text-generation"},
            {"id": "gemini-1.5-pro", "name": "Gemini 1.5 Pro", "type": "text-generation"},
        ]
        return jsonify({
            "status": "success",
            "models": models,
            "default": MODEL_ID
        }), 200
    except Exception as e:
        logger.error(f"â?åˆ—å‡ºæ¨¡å‹å¤±è´¥: {e}")
        return jsonify({"status": "error", "error": str(e)}), 500

@app.errorhandler(404)
def not_found(error):
    """404 é”™è¯¯å¤„ç†"""
    return jsonify({
        "status": "error",
        "error": "Endpoint not found",
        "available_endpoints": ["/", "/health", "/api/chat", "/api/models"]
    }), 404

@app.errorhandler(500)
def internal_error(error):
    """500 é”™è¯¯å¤„ç†"""
    logger.error(f"â?å†…éƒ¨æœåŠ¡å™¨é”™è¯? {error}")
    return jsonify({
        "status": "error",
        "error": "Internal server error"
    }), 500

if __name__ == '__main__':
    logger.info(f"ğŸš€ å¯åŠ¨ moda.STUDIO åç«¯æœåŠ¡ v2.0")
    logger.info(f"   æ¨¡å‹: {MODEL_ID}")
    logger.info(f"   ç¼“å­˜: {MAX_CACHE_SIZE} items, TTL {CACHE_TTL}s")
    logger.info(f"   é™åˆ¶: {MAX_REQUESTS_PER_MINUTE} req/min")
    app.run(host='0.0.0.0', port=PORT, debug=False)

