version: '3.8'

services:
  db:
    image: mysql:8.0
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: secret
      MYSQL_DATABASE: magicai
      MYSQL_USER: magicai
      MYSQL_PASSWORD: secret
    volumes:
      - db_data:/var/lib/mysql
    ports:
      - "3307:3306"
    networks:
      - magicai-network

  redis:
    image: redis:7
    restart: always
    ports:
      - "6379:6379"
    networks:
      - magicai-network

  web:
    image: php:8.2-apache
    restart: always
    depends_on:
      - db
      - redis
    working_dir: /var/www/html
    environment:
      APP_ENV: production
      APP_DEBUG: 'false'
      APP_URL: https://modamoda.shop
      DB_HOST: db
      DB_DATABASE: magicai
      DB_USERNAME: magicai
      DB_PASSWORD: secret
      BROADCAST_DRIVER: redis
      QUEUE_CONNECTION: redis
      CACHE_DRIVER: redis
      SESSION_DRIVER: redis
      VERTEX_AI_BACKEND_URL: http://python-backend:8080
    volumes:
      - "./server:/var/www/html"
      - "./server/public:/var/www/html/public"
    networks:
      - magicai-network
    ports:
      - "8000:80"

  nginx:
    image: nginx:latest
    restart: always
    depends_on:
      - web
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - "./server/public:/var/www/html/public:ro"
      - "./nginx.conf:/etc/nginx/nginx.conf:ro"
      - "./certs/certs:/etc/nginx/certs:ro"
    environment:
      SERVER_NAME: modamoda.shop
    networks:
      - magicai-network

  python-backend:
    image: python:3.11-slim
    restart: always
    depends_on:
      - db
    working_dir: /app
    environment:
      GCP_PROJECT_ID: test-project
      GCP_REGION: us-central1
      MODEL_ID: gemini-2.0-flash
      PORT: 8080
    volumes:
      - "./backend-service:/app"
    ports:
      - "8080:8080"
    networks:
      - magicai-network
    command: "python app.py"

  # ========== Ollama 本地 AI 模型服務 ==========
  ollama:
    image: ollama/ollama:latest
    restart: always
    ports:
      - "11434:11434"
    environment:
      # GPU 支持（如果有 NVIDIA GPU，取消註釋）
      # CUDA_VISIBLE_DEVICES: "0"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - magicai-network
    # 預載入模型（可選）
    # 運行此容器後，執行: docker exec magicai-ollama ollama pull llama2
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

volumes:
  db_data:
  ollama_data:

networks:
  magicai-network:
    driver: bridge
